
Manual
======

| R. D. McMichael rmcmichael@nist.gov
| National Institute of Standards and Technology
| Gaithersburg, MD USA
| March 29, 2020

Contents
--------

-  `Introduction <#introduction>`__
-  `Philosophy and goals <#philosophy-and-goals>`__
-  `Requirements <#requirements-for-users>`__
-  `Demos <#demos>`__
-  `Theory of Operation <#theory-of-operation>`__

Introduction
------------

This manual describes an implementation of optimal Bayesian experimental
design methods to control measurement settings in order to efficiently
determine model parameters. In situations where parametric models would
conventionally be fit to measurement data in order to obtain model
parameters, these methods offer an adaptive measurement strategy capable
of reduced uncertainty with fewer required measurements. These methods
are therefore most beneficial in situations where measurements are
expensive in terms of money, time, risk, labor or discomfort. The price
for these benefits lies in the complexity of automating such
measurements and in the computational load required. It is the goal of
this package to assist potential users in overcoming at least the
programming hurdles.

Optimal Bayesian experimental design is not new, at least not in the
statistics community. A review paper from 1995 by `Kathryn Chaloner and
Isabella Verinelli <https://projecteuclid.org/euclid.ss/1177009939>`__
reveals that the basic methods had been worked out in preceding decades.
The methods implemented here closely follow `Xun Huan and Youssef M.
Marzouk <http://dx.doi.org/10.1016/j.jcp.2012.08.013>`__ which
emphasizes simulation-based experimental design. Optimal Bayesian
experimental design is also an active area of research.

There are at least three important factors that encourage application of
these methods today. First, the availability of flexible, modular
computer languages such as Python. Second, availability of cheap
computational power. Most of all though, an increased awareness of the
benefits of code sharing and reuse is growing in scientific communities.

Philosophy and goals
--------------------

+---------------------------------------+
|    If it sounds good, it is good.     |
|                    -- Duke Ellington  |
+---------------------------------------+

Jazz legend Duke Ellington was talking about music, where it’s all about
the sound. For this package, it’s all about being useful. The goals are
modest: to adapt some of the developments in optimal Bayeseian
experimental design research for practical use in laboratory settings,
giving users tools to make better measurements.  This project adopts a
"runs good" philosophy:

-  If it’s a struggle to use, it can’t run good.
-  If technical jargon is a barrier, it can’t run good
-  If the user finds it useful, it runs good.
-  If it runs good, it is good.

Requirements for users
----------------------

It takes a little effort to get this software up and running. Here’s
what a user will need to supply to get started.

1. Python 3.x with the ``numpy`` python package
2. An experiment that yields measurement results with uncertainty
   estimates.
3. A reliable parametric model for the experiment, typically a function with
   parameters to be determined.
4. A working knowledge of Python programming, enough to follow examples
   and program a model function.

Demos
-----

Locating a Lorentzian peak
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. figure:: _images/demoLorentzfig1.png
   :alt: Comparison of measure-then-fit and OBED method

   Comparison of measure-then-fit and OBED method

Figure 1: A comparison of measure-then-fit (left) and optimal Bayesian
experimental design (right). Both methods measure the same “true” peak
with Gaussian noise added (:math:`\sigma = 1`) independently. The peak
parameters are selected randomly: center between 2 and 4, height between
2 and 5, background between -1 and 1 and peak width between 0.1 and 0.3.
On the left, 30 evenly-spaced “measurements” are made and fit using
``scipy.optimize.curve_fit()``. A curve using the best-fit parameters is
plotted for comparison with the true curve. The diagonal element of the
covariance matrix is taken as the square of the uncertainty. On the
right, the optimal Bayes experimental design method is used
sequentially. Iterations stop when the standard deviation of the ``x0``
peak center distribution is less than the uncertainty of the fit on the
left. Green curves are generated from random draws from the parameter
distribution at the stopping point. Typical runs of this example require
approximately 1/4 to 1/2 as many measurements as the measure-then-fit
method. See ``Demos/demoLorentzian.py``.

Locating a Lorentzian peak
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. figure:: _images/sequentialLorentzian.png
    :alt: Behavior and results of a sequential Bayesian experimental design
        approach to measurement a Lorentzian peak.

Figure 1.  Behavior and results of a sequential Bayesian experimental design
approach to measurement of a Lorentzian peak. The unknown parameters are
the peak position and amplitude and the background level, and the peak
width is fixed. Panel (a) shows the true curve in red and the simulated
measurement data after 500 measurements. The signal to noise ratio is
approximately 1.0.  The algorithm focuses measurement resources near the
peak as quantified by the histogram in (c). Measurements far from the peak
reduce uncertainty in the background level.  Panels (b) and (d) illustrate
how the process evolves.  (b) plots the standard deviation of the peak
center, and (d) plots the measurement settings. For the first 50-ish
measurements, the settings are scattered and the standard deviation drops
slowly. After 50-ish measurements, the standard deviation drops quickly and
the settings concentrate on the peak level.  At around 150 measurements, the
system enters a steady-state pattern of settings near the peak with less
frequent measurements of the background.  In this stage, the standard
deviation typically reduces proportional to :math:`1/\sqrt{N}`.


Measurement speedup
~~~~~~~~~~~~~~~~~~~~~~~~

.. figure:: _images/rootN.png
   :alt: Mechanism of speedup


Figure 2: A different view of the Lorentzian peak problem, contrasting
efficiency differences between methods. The left panel shows results
after 3000 measurements of a spectrum. The simulated experimental noise
is Gaussian with standard deviation :math:`\sigma = 1`. In the “average
& fit” method, 100 simulated measurements are averaged at each of 30
points, yielding a uniform uncertainty of
:math:`\sigma_y = 1/\sqrt{100}`. In the OptBayesExpt method, the
algorithm focuses attention on the sides of the peak, as shown in the
histogram. The symbol areas are proportional to the weights of the data
points, :math:`1/\sigma^2`. The smallest points correspond to
:math:`\sigma_y = 0.258`, and the largest to
:math:`\sigma_y \approx 0.037.`

The right panel plots the evolution of uncertainty in the peak center
:math:`x_0` with the number of accumulated measurements. Plotted values
summarize the results of 100 runs, each with 3000 OptBayesExpt
measurments and 15 000 average & fit measurements. Dark lines plot the difference between 'best'
values and the 'true' values as an rms error.  Faint lines plot the standard deviation and
standard uncertainty of individual runs.  Both the average & fit
technique and the OptBayesExpt method scale like
:math:`\sigma_{x0} \propto 1/\sqrt{N}` (:math:`N` = measurement number)
for large :math:`N`, but the OptBayesExpt requires approximately 4
times fewer measurements to achieve the same uncertainty.

Details: The average and fit method used ``scipy.optimize.curve_fit()``. The uncertainty plotted
here is the square root of the diagonal element in the covariance matrix. For both methods, the peak
height, background level and peak center are treated as unknowns, and
the half-width line width is fixed at 0.3. See ``demos/fit_vs_obe_makedata.py`` and
``demos/fit_vs_obe_plots.py``

Tuning a :math:`\pi` pulse
~~~~~~~~~~~~~~~~~~~~~~~~~~

.. figure:: _images/pipulsefig.png
   :alt: measurements of transition probability vs pulse length and detuning

   measurements of transition probability vs pulse length and detuning

Figure 3: A :math:`\pi` pulse is a method of inverting spins that is
frequently used in nuclear magnetic resonance (NMR and MRI) and pulsed
electron paramagnetic resonance (EPR). In order to be accurate, the
duration and frequency of the radio-frequency pulse must be tuned. On
the left, the background image displays the model photon counts for
optically detected spin manipulation for different frequency detunings
and pulse lengths. White indicates the expected result for spin up and
black, spin down. Points indicate simulated measurement settings, with
sequence in order from white to dark red. Simulated measurements have
1\ :math:`\sigma` uncertainties of :math:`\sqrt{N} \approx 316`. The right panel displays the
evolution of the probability distribution function with the number “N”
of measurements. See ``Demos.pipulse.py``.

Slope Intercept
~~~~~~~~~~~~~~~

.. figure:: _images/slopeintercept.png
   :alt: Straight line measurement examples

   Straight line measurement examples

Figure 4: This demo uses a straight line model, a case where the “best”
measurement settings are known in advance; measurements at the ends of a
line are the most effective at reducing uncertainty in the slope and
intercept values. For reassurance that the straight line model is
appropriate, some measurements in the in the middle of the span might
also be desired. The OptBayesExpt class provides two methods for
flexibility in measurement selection. The ``opt_setting()`` method
selects the setting with the highest *utility* :math:`\max[U(x)]`. The
first panel shows that it behaves as expected, choosing measurements at
the ends of the line. The ``good_setting()`` method is more flexible,
selecting settings with a probability based on the *utility* and the
``pickiness`` parameter. The 2nd through 4th panels show that the
``good_setting()`` algorithm selects more diverse setting values as the
``pickiness`` is reduced. Note also that the standard deviations
increase from left to right as measurement resources are diverted away
from reducing uncertainty. Each run uses 100 points. See
``demos/line_plus_noise.py``.

Theory of operation
-------------------
The optimal Bayes experimental design method incorporates two main jobs,
which we can describe as “learning early” and “making good decisions”

Learning early
~~~~~~~~~~~~~~

By interpreting measurement data as soon as it becomes available, sequential Bayesian
experimental design gains a critical advantage over the traditional measure-then-fit design.
With a measure-then-fit strategy, we get information about parameters only at the very end of the
process, after all the measurements and fitting are done. In contrast, the optimal Bayesian
experimental design method updates our parameter knowledge with each measurement result, so
that information-based decisions can be made as data is collected.

The process of digesting new data is Bayesian inference, which frames
parameter knowledge in terms of a probability distribution :math:`p(\theta)` for an array of
parameters :math:`\theta = [ \theta_0, \theta_1, ...]`. The familiar notation :math:`a\pm \sigma`
is often a shorthand description of a Gaussian probability distribution. A broad distribution
:math:`p(\theta)` corresponds to large uncertainty, and if :math:`p(\theta)` is a narrow
distribution, the uncertainty is small.

When new measurement results :math:`m` are taken in to account, there will be a new,
revised probability distribution :math:`p(\theta|m)`. The vertical bar in the notation
:math:`p(\theta|m)` indicates a conditional probability, so :math:`p(\theta|m)` is the
distribution of :math:`\theta` values *given* :math:`m`.

Bayes’ rule provides

.. math::  p(\theta|m) = \frac{p(m|\theta) p(\theta)}{p(m)}. 

All of the terms here have technical names. The left side is the
*posterior* distribution, i.e. the distribution of parameters
:math:`\theta` after we include :math:`m`. On the right, distribution
:math:`p(\theta)` is the *prior*, representing what we knew about the
parameters :math:`\theta` before the measurement. In the denominator,
:math:`p(m)` is called the *evidence*, but because it has no :math:`\theta`
dependence, it functions just a normalizing constant in this situation.
As wrong as this sounds, we will ignore the *evidence*.

The term that requires attention is in the numerator; :math:`p(m|\theta)` is called the
*likelihood*. It’s the probability of getting measurement :math:`m`
given variable parameter values :math:`\theta`.  Less formally, the *likelihood* answers the
question: "How well does the model explain the measured value
:math:`m`, when the model uses different parameter values :math:`\theta`?"

In practice, :math:`m_i` will be a fixed measurement result to “plug in” for :math:`m`. It’s
important to keep sight of the fact that :math:`p(m_i|\theta)` is still a function of
theta. Conceptually, we can try out different parameter values in our
model to produce a variety of measurement predictions. Some parameter
values (the more likely ones) will produce model values closer to
:math:`m_i` and for other parameters, (the less likely ones), model
model value will be further away.

The ``optbayesexpt.OptBayesExpt`` class requires the user to report a measurement record
:math:`m_i` that includes the measurement settings :math:`x_i`, the “value” :math:`y_i`, and
uncertainty :math:`\sigma_i`. Together, :math:`y_i` and :math:`\sigma_i` are more than fixed
numbers; they
are shorthand for a probability that a noise-free measurement would yield a value :math:`y`.
:math:`y` given a mean value :math:`y_i`. If this distribution is symmetric, like a Gaussian, for
example, then :math:`p(y|y_i, \sigma_i) = p(y_i|y, \sigma_i)`, the probability of measuring
:math:`y_i` given a mean value :math:`y` that’s provided by the experimental model :math:`y=y
(x_i,\theta)`.

.. math::  p(m_i|\theta) = \frac{1}{\sqrt{2\pi}\sigma_i}
            \exp\left[-\frac{[y_i - y(x_i, \theta)]^2 }{ 2\sigma_i^2 } \right].

With this *likelihood*, Bayes theorem provides the updated  :math:`p(\theta|m_i)`.
Then, another measurement :math:`m_j` can update :math:`p(\theta|m_i)` to
:math:`p(\theta|m_j, m_i, \ldots)` and so on. In order to keep the
notation readable, we’ll adopt a convention that :math:`p(\theta)`
always represents the most up-to-date parameter distribution that we
have.

This approach assumes that our model function :math:`y(x, \theta)` is a good description of our
system, and that the measurement noise is Gaussian with standard deviation
:math:`\sigma_i`. On one hand we have to admit that these assumptions don’t allow us to
address all important cases. On the other hand, these are the same
assumptions we often make in doing least-squares curve fitting.

The method described above puts the responsibility for determining
measurement uncertainty on the measurement reporter, but as an experimental result, uncertainty is
generally a measurement output, not an input.  If uncertainty is a parameter to be determined, it
enters the process through the likelihood function given above, but it is not part of the model
function :math:`y(x_i, \theta)`. See ``demos/line_plus_noise.py`` for an example.

Making good decisions
~~~~~~~~~~~~~~~~~~~~~

The next important job in the process is figuring out good measurement
settings. The goal is to make the parameter probability distribution
:math:`p(\theta)` narrow while minimizing cost. More formally, the
challenge is to develop a *utility function* :math:`U(x)` that helps us
to predict and compare the relative benefits of measurements made with
different possible experimental settings :math:`x`.

Qualitatively, the mechanism for choosing measurement values hinges on the model's connection
between parameter values :math:`\theta` and measurement results :math:`y`.
Consider a sampling of several sets of parameter values :math:`\theta_i`.  With these parameter
sets, the model can produce a collection of output curves :math:`y_i(x)`, and generally these
curves will be closer together for some settings :math:`x` and further apart for others.
Intuitively, little would be learned by measuring at :math:`x` values where the curves
agree.  Instead, it would do the most good to “pin down” the results with a measurement at an
:math:`x` where the predicted :math:`y_i(x)` curves disagree.

By drawing samples from the updated parameter distribution :math:`p(\theta)` the mechanism above
focuses attention on the relevant parts of parameter space, and through the model to relevant
settings. Or, stated slightly differently, using an updated parameter distribution helps to avoid
wasting measurement resources on low-impact measurements.

Estimate benefits
^^^^^^^^^^^^^^^^^

To translate such a qualitative argument into code, "doing the most good"
must be defined more precisely in terms of desired changes in
the parameter distribution :math:`p(\theta)`. Usually, the goal in
determining model parameters is to get unambiguous results with small uncertainty. The *information
entropy* provides a measure of something like a probability distribution. The information entropy
of a probability distribution :math:`p(a)` is defined as

.. math::  E = -\int da\; p(a)\; \ln[p(a)].

Note that the integrand above is zero for both :math:`p(a) = 1` and
:math:`p(a)=0`. It’s the intermediate values encountered in a
spread-out distribution where the information entropy accumulates. For
common distributions, like rectangular or Gaussian, that have
characteristic widths :math:`w` the entropy goes like :math:`\ln(w) + C` with :math:`C` values
depending on the shape of the distribution.

Now we can define "doing the most good" in terms of how much
entropy change :math:`E`\ (*posterior*) - :math:`E`\ (*prior*) we
might get for predicted measurement values :math:`y` at different
settings :math:`x`. Actually, we use something slightly
different called the Kulback-Liebler divergence:

.. math::  D^{KL}(y,x) = \int d\theta\; p(\theta |y,x)
    \ln \left[ \frac{p(\theta | y,x)}{p(\theta)}\right].

In this expression :math:`p(\theta | y,x)` is a speculative parameter
distribution we would get if we happened to measure a value :math:`y`
using settings :math:`x`. By itself, :math:`D^{KL}(y,x)` doesn’t work
as a utility function :math:`U(x)` because it depends on this
arbitrary possible measurement value :math:`y`. So we need to average
:math:`D^{KL}`, weighted by the probability of measuring :math:`y`.

.. math::  U(x) \propto \int dy \int d\theta\; p(y|x) p(\theta |y,x)
    \ln \left[ \frac{p(\theta | y,x)}{p(\theta)}\right].

With two applications of Bayes rule and some rearrangement this expression for
:math:`U(x)` can be rewritten as the difference between two information entropy-like terms:

:Term 1: The information entropy of :math:`p(y|x)`, the distribution of
        measurement values expected at setting :math:`x`. Importantly this distribution
        includes likely variations of :math:`\theta.` Explicitly,

        .. math::  p(y|x) = \int d\theta'\; p(\theta') p(y|\theta',x)

        Qualitatively, this term is the information entropy of the predicted measurement values
        including both measurement noise and the effects of parameter uncertainty.

:Term 2: The other term is the information entropy of :math:`p(y|\theta,x)` the measurement value
        distribution when :math:`\theta` and :math:`x` are fixed, i.e. the entropy of just the
        measurement noise distribution. The entropy of this distribution is averaged over
        :math:`\theta` values.

        .. math::  \int d\theta\; p(\theta) \int dy\; p(y|\theta,x) \ln [ p(y|\theta, x) ]

Term 1 is the entropy of the :math:`\theta`-averaged :math:`y`
distribution and Term 2 is the :math:`\theta` average of the entropy of
the :math:`y` distribution. Loosely, Term 1 is a measure of the spread
in :math:`y` values due to both measurement noise and likely parameter
variations, while term 2 is (mostly) just the measurement noise.

An accurate calculation of :math:`U(x)` is a big job, requiring integrals over all
parameter space and also all possible measurement outcomes, once for every possible setting.
Fortunately, in keeping with the “runs good” project philosophy, accuracy is not required.
All we require of an approximate utility function is that provides a guide for non-stupid
decisions. It is not critical that the absolute best measurement choice is made
every single time.  It is only necessary to know if there are values of :math:`x`
where :math:`U (x)` is large compared to other
:math:`x`.  Even if we don’t choose the absolute best setting,
a “pretty good” choice will do more good than an uninformed choice.

Consider an approximate calculation of :math:`U^*(x)`
where all of the distributions are assumed to be normal (Gaussian). The information
entropy of the normal distribution has a term that goes like
:math:`\ln`\ (width). Term 1 from above is a convolution of the
measurement noise distribution (width = :math:`\sigma_y` and the
distribution of model :math:`y` values (width =
:math:`\sigma_{y,\theta}`) that reflects the connection to the parameter
distribution. A property of normal distributions is that a convolution
of normal distributions is another normal distribution with width =
:math:`\sqrt{\sigma_{y,\theta}^2 + \sigma_y^2}`. Under the assumption of
normal distributions, we now have an approximate utility function

.. math::

    U^*(x) \propto \approx \ln(\sqrt{\sigma_\theta^2 + \sigma_y^2}) - \ln(\sigma_y)
            = \frac{1}{2}\ln\left[\frac{\sigma_{y,\theta}(x)^2}{\sigma_y(x)^2}+1\right]

This approximation has some reasonable properties. The dependence on
:math:`\sigma_{y,\theta}` matches our initial intuition that
high-utility parameters are those where measurements vary a lot due to
parameter variations. The dependence on measurement noise
:math:`\sigma_y` also has an intuitive interpretation: that it’s less
useful to make measurements at settings :math:`x` where the
instrumental noise is larger. This approximate utility function is
also positive, i.e. more data helps narrow a distribution.

Estimate the costs
^^^^^^^^^^^^^^^^^^

.. warning:: Don't forget the cost

It is also important to consider the cost associated with different settings,
including possible "travel costs" of moving between settings.  The *utility* should be divided
by the cost to make setting decisions based on benefit/cost ratios rather than on benefit alone.

In principle, the cost function is as important as the entropy change term detailed above, but
there's not much to write about it.

